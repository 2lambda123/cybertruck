{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of dataloader usage. Unsure of how to load skin segmentation data at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasest dir should look like:\n",
    "\n",
    "\n",
    "# â”œâ”€â”€ data\n",
    "# â”‚   â”œâ”€â”€ v1_cam1_no_split\n",
    "# |      â”œâ”€â”€ Adjust Radio\n",
    "# |          â”œâ”€â”€ 1.jpg\n",
    "# â”‚      â”œâ”€â”€ Drink\n",
    "# |           â”œâ”€â”€ 1.jpg\n",
    "# |      â”œâ”€â”€ Test_data_list.csv\n",
    "# |      â”œâ”€â”€ Train_data_list.csv\n",
    "# |\n",
    "# â”‚   â”œâ”€â”€ v2_cam1_cam2_ split_by_driver\n",
    "# |      â”œâ”€â”€ Camera 1\n",
    "# |          â”œâ”€â”€ test\n",
    "# |          â”œâ”€â”€ train\n",
    "# â”‚      â”œâ”€â”€ Camera 2\n",
    "# |          â”œâ”€â”€ test\n",
    "# |          â”œâ”€â”€ train\n",
    "# |      â”œâ”€â”€ skin_nonskin_pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on how to (separately) export detector and cnns models to ONNX formats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.hands_cnn import Hands_VGG16\n",
    "from wrappers.hands_wrapper import *\n",
    "import torch.onnx\n",
    "import argparse\n",
    "\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "args.add_argument('--model', type=str, default='hands_vgg')\n",
    "args.add_argument('--distributed', type=bool, default=False)\n",
    "\n",
    "args.add_argument('--resume_path', type=str, default=None)\n",
    "args.add_argument('--resume_last_epoch', type=bool, default=False)\n",
    "\n",
    "args.add_argument('--hidden_units', type=int, default=128)\n",
    "args.add_argument('--freeze', type=bool, default=True)\n",
    "args.add_argument('--batch_size', type=int, default=64)\n",
    "args.add_argument('--epochs', type=int, default=30)\n",
    "\n",
    "args.add_argument('--lr', type=float, default=1e-3)\n",
    "args.add_argument('--num_frozen_params', type=int, default=30)\n",
    "args.add_argument('--dropout', type=float, default=0.5)\n",
    "args.add_argument('--optimizer', type=str, default='SGD')\n",
    "args.add_argument('--weight_decay', type=float, default=0.0)\n",
    "args.add_argument('--scheduler', action='store_true')\n",
    "\n",
    "args.add_argument('--save_period', type=int, default=5)\n",
    "args.add_argument('--transform', type=bool, default=True) \n",
    "args.add_argument('--device', type=str, default='cuda')\n",
    "\n",
    "args.add_argument('--data_dir', type=str, default='data/v2_cam1_cam2_split_by_driver')\n",
    "args.add_argument('--model_dir', type=str, default='cnn/hands_models')\n",
    "args.add_argument('--detector_path', type=str, default='path/to/yolo/weights')\n",
    "\n",
    "args = args.parse_args()\n",
    "\n",
    "hands_model = Hands_VGG16(args, 10).to('cuda')\n",
    "x = torch.randn(64, 3, 224, 224).to('cuda')\n",
    "\n",
    "hands_model.load_state_dict(torch.load(\"/home/ron/Classes/CV-Systems/cybertruck/cnn/hands_models/vgg/epoch60_11-16_03:44:44.pt\"))\n",
    "torch.onnx.export(hands_model, x, 'hands_cnn.onnx', export_params=True, opset_version=17, do_constant_folding=True, input_names = ['input'], \n",
    "                  output_names = ['output'], dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208 ðŸš€ Python-3.10.13 torch-2.1.0+cu121 CPU (12th Gen Intel Core(TM) i7-12700H)\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'detection/hands_detection/runs/detect/best/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.5s, saved as 'detection/hands_detection/runs/detect/best/weights/best.onnx' (11.7 MB)\n",
      "\n",
      "Export complete (2.2s)\n",
      "Results saved to \u001b[1m/home/ron/Classes/CV-Systems/cybertruck/detection/hands_detection/runs/detect/best/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=detection/hands_detection/runs/detect/best/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=detection/hands_detection/runs/detect/best/weights/best.onnx imgsz=640 data=/home/ron/Classes/CV-Systems/cybertruck/detection/hands_detection/egohands.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'detection/hands_detection/runs/detect/best/weights/best.onnx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "hands_detector = YOLO('detection/hands_detection/runs/detect/best/weights/best.pt')\n",
    "hands_detector.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybertruck-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

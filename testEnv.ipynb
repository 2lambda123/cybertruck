{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of dataloader usage. Unsure of how to load skin segmentation data at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasest dir should look like:\n",
    "\n",
    "\n",
    "# ├── data\n",
    "# │   ├── v1_cam1_no_split\n",
    "# |      ├── Adjust Radio\n",
    "# |          ├── 1.jpg\n",
    "# │      ├── Drink\n",
    "# |           ├── 1.jpg\n",
    "# |      ├── Test_data_list.csv\n",
    "# |      ├── Train_data_list.csv\n",
    "# |\n",
    "# │   ├── v2_cam1_cam2_ split_by_driver\n",
    "# |      ├── Camera 1\n",
    "# |          ├── test\n",
    "# |          ├── train\n",
    "# │      ├── Camera 2\n",
    "# |          ├── test\n",
    "# |          ├── train\n",
    "# |      ├── skin_nonskin_pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import *\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "v1_train = V1Dataset(csv_path='data/v1_cam1_no_split/Train_data_list.csv', images_folder='data/v1_cam1_no_split', \n",
    "                     transform=transform)\n",
    "v1_test = V1Dataset(csv_path='data/v1_cam1_no_split/Test_data_list.csv', images_folder='data/v1_cam1_no_split', \n",
    "                    transform=transform)\n",
    "\n",
    "v2_train = V2Dataset(cam1_path='data/v2_cam1_cam2_ split_by_driver/Camera 1/train', \n",
    "                     cam2_path='data/v2_cam1_cam2_ split_by_driver/Camera 2/train', transform=transform)\n",
    "v2_test = V2Dataset(cam1_path='data/v2_cam1_cam2_ split_by_driver/Camera 1/test', \n",
    "                    cam2_path='data/v2_cam1_cam2_ split_by_driver/Camera 2/test', transform=transform)\n",
    "\n",
    "v1_trainloader = DataLoader(v1_train, batch_size=32, shuffle=True)\n",
    "v1_testloader = DataLoader(v1_test, batch_size=32, shuffle=True)\n",
    "\n",
    "v2_trainloader = DataLoader(v2_train, batch_size=32, shuffle=True)\n",
    "v2_testloader = DataLoader(v2_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of v1_trainloader: 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ron/miniconda3/envs/cybertruck-env/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'Len of v1_trainloader: {len(v1_trainloader)}')\n",
    "for images, label in v1_trainloader:\n",
    "    print(images.shape)\n",
    "    print(label.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of v1_trainloader: 136\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'Len of v1_trainloader: {len(v1_testloader)}')\n",
    "for images, label in v1_testloader:\n",
    "    print(images.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of v2_trainloader: 393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'Len of v2_trainloader: {len(v2_trainloader)}')\n",
    "for images, label in v2_trainloader:\n",
    "    print(images.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of v1_trainloader: 61\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'Len of v1_trainloader: {len(v2_testloader)}')\n",
    "for images, label in v2_testloader:\n",
    "    print(images.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybertruck-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Hands CNN and Detector from ONNX to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands CNN + Detector Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# import tensorflow as tf\n",
    "# import onnx_tf\n",
    "import os\n",
    "import onnx\n",
    "import numpy as np\n",
    "\n",
    "root_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'face_cnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Prepare the Onnx model for tensorflow and then convert to tflite\n",
    "\n",
    "https://medium.com/@zergtant/convert-pytorch-model-to-tf-lite-with-onnx-tf-232a3894657c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_onnx_to_tflite(model_name):\n",
    "    # Load  ONNX model\n",
    "    onnx_model = onnx.load(f'{model_name}.onnx')\n",
    "\n",
    "    # Convert ONNX model to TensorFlow format\n",
    "    tf_model = onnx_tf.backend.prepare(onnx_model)\n",
    "    # Export  TensorFlow  model \n",
    "    tf_model.export_graph(f'{model_name}.tf')\n",
    "\n",
    "    # Then convert from TF to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(f'{model_name}.tf')\n",
    "    tflite_model = converter.convert()\n",
    "    open(f'{model_name}.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "convert_onnx_to_tflite(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test the converted model\n",
    "\n",
    "https://www.tensorflow.org/lite/guide/inference\n",
    "https://www.tensorflow.org/lite/guide/inference#run_inference_with_dynamic_shape_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, this model takes a batch size of 1. Which works out well since we want real time performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tflite_model(model_name):\n",
    "    model_filename = f\"{model_name}.tflite\"\n",
    "\n",
    "    # Load the TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_filename)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test the model on random input data.\n",
    "    input_shape = input_details[0]['shape']\n",
    "    print(input_shape)\n",
    "    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    print(output_data)\n",
    "\n",
    "test_tflite_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dynamic_batches_tflite(model_name):\n",
    "    model_filename = f\"{model_name}.tflite\"\n",
    "\n",
    "    # Load the TFLite model in TFLite Interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_filename)\n",
    "\n",
    "    input_shape = interpreter.get_input_details()[0]['shape']\n",
    "    input_shape[0] = 5 # Set to batch size of 5 instead of the default 1\n",
    "\n",
    "    # Resize input shape for dynamic shape model and allocate tensor\n",
    "    interpreter.resize_tensor_input(interpreter.get_input_details()[0]['index'], input_shape)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    print(input_details) # Here we can see the input size has changed to (5, 3, 299, 299) \n",
    "    print(output_details) # The output has the corresponding changes\n",
    "\n",
    "    input_data = np.array(np.random.random_sample((5, 3, 224, 224)), dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    print(output_data)\n",
    "\n",
    "test_dynamic_batches_tflite(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Convert the Detector from Pytorch to TFLite with Ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_path = 'best_hands_detector.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(detector_path)  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format='tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Usage of TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = r\"D:\\Programming\\cybertruck\\conversions\\best_hands_detector_saved_model\\best_hands_detector_float32.tflite\"\n",
    "\n",
    "# Load the TFLite model in TFLite Interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details() # (1, 640, 640, 3) Notice that the RGB channel is at the end for the detector..\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_data = np.array(np.random.random_sample(input_details[0]['shape']), dtype=np.float32)\n",
    "\n",
    "print(input_details) # Here we can see the input size has changed to (5, 3, 299, 299) \n",
    "print(output_details) # The output has the corresponding changes\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output)\n",
    "\n",
    "# Obtaining output results\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "output = output[0]\n",
    "output = output.T\n",
    "\n",
    "boxes_xywh = output[..., :4] #Get coordinates of bounding box, first 4 columns of output tensor\n",
    "scores = np.max(output[..., 5:], axis=1) #Get score value, 5th column of output tensor\n",
    "classes = np.argmax(output[..., 5:], axis=1) # Get the class value, get the 6th and subsequent columns of the output tensor, and store the largest value in the output tensor.\n",
    "\n",
    "print(boxes_xywh)\n",
    "print(scores)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Detector Inference with TFLite\n",
    "\n",
    "https://github.com/ultralytics/ultralytics/issues/4827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "tflite_model_path = r\"D:\\Programming\\cybertruck\\conversions\\best_hands_detector_saved_model\\best_hands_detector_float32.tflite\"\n",
    "image_path = r\"D:\\Programming\\cybertruck\\conversions\\bus.jpg\"\n",
    "\n",
    "def run_model_and_draw_results(model_path, image_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Obtain the height and width of the corresponding image from the input tensor\n",
    "    image_height = input_details[0]['shape'][1] # 640\n",
    "    image_width = input_details[0]['shape'][2] # 640\n",
    "\n",
    "    # Image Preparation\n",
    "    image = Image.open(image_path)\n",
    "    image_resized = image.resize((image_width, image_height)) # Resize the image to the corresponding size of the input tensor and store it in a new variable\n",
    "\n",
    "    image_np = np.array(image_resized) #\n",
    "    image_np = np.true_divide(image_np, 255, dtype=np.float32) \n",
    "    image_np = image_np[np.newaxis, :]\n",
    "\n",
    "    # inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Obtaining output results\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output = output[0]\n",
    "    output = output.T\n",
    "\n",
    "    boxes_xywh = output[..., :4] #Get coordinates of bounding box, first 4 columns of output tensor\n",
    "    scores = np.max(output[..., 5:], axis=1) #Get score value, 5th column of output tensor\n",
    "    classes = np.argmax(output[..., 5:], axis=1) # Get the class value, get the 6th and subsequent columns of the output tensor, and store the largest value in the output tensor.\n",
    "\n",
    "    # Threshold Setting\n",
    "    threshold = 0.3\n",
    "\n",
    "    # Bounding boxes, scores, and classes are drawn on the image\n",
    "    draw = ImageDraw.Draw(image_resized)\n",
    "\n",
    "    for box, score, cls in zip(boxes_xywh, scores, classes):\n",
    "        if score >= threshold:\n",
    "            x_center, y_center, width, height = box\n",
    "            x1 = int((x_center - width / 2) * image_width)\n",
    "            y1 = int((y_center - height / 2) * image_height)\n",
    "            x2 = int((x_center + width / 2) * image_width)\n",
    "            y2 = int((y_center + height / 2) * image_height)\n",
    "\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "            text = f\"Class: {cls}, Score: {score:.2f}\"\n",
    "            draw.text((x1, y1), text, fill=\"red\")\n",
    "\n",
    "    display(image_resized)\n",
    "\n",
    "run_model_and_draw_results(tflite_model_path, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Model for Pytorch Lite (PTL) Android deployment\n",
    "\n",
    "https://pytorch.org/mobile/android/\n",
    "\n",
    "For YOLO detector we use the export function to convert to torchscript, then we convert the torchscript into .ptl for android deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208 🚀 Python-3.10.13 torch-2.1.0+cu121 CPU (12th Gen Intel Core(TM) i7-12700H)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.1.0+cu121...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 0.8s, saved as '/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.torchscript' (11.9 MB)\n",
      "\n",
      "Export complete (2.2s)\n",
      "Results saved to \u001b[1m/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.torchscript imgsz=640 data=datasets/data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.torchscript'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, yaml\n",
    "from ultralytics import YOLO\n",
    "import torchvision\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model = YOLO(r\"/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.pt\")  # load a custom trained model\n",
    "\n",
    "model.export(format='torchscript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchscript_model = torch.jit.load('/home/ron/Classes/CV-Systems/cybertruck/detection/face_detection/weights/yolov8n-face.torchscript')\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "torchscript_model_optimized.eval()\n",
    "\n",
    "\n",
    "scripted_module = torch.jit.script(torchscript_model_optimized)\n",
    "scripted_module._save_for_lite_interpreter(\"./best_face_detector.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 8400])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchscript_lite_model = torch.jit.load('./best_hands_detector.ptl')\n",
    "torchscript_lite_model.eval()\n",
    "\n",
    "x = torch.rand(1, 3, 640, 640)\n",
    "\n",
    "output = torchscript_lite_model(x)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting CNNs to Pytorch Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "\n",
    "root_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from cnn.face_cnn import Face_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HandArgs:\n",
    "#     def __init__(self):\n",
    "#         self.freeze = True\n",
    "#         self.num_frozen_params = 30\n",
    "#         self.dropout = 0.35\n",
    "\n",
    "# hand_args = HandArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ron/miniconda3/envs/cyber/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "face_model = Face_CNN(None).to('cuda')\n",
    "x = torch.randn(5, 3, 299, 299).to('cuda')\n",
    "face_model.load_state_dict(torch.load(r\"/home/ron/Classes/CV-Systems/cybertruck/cnn/face_models/face/SGD/epoch10_11-28_10:50:06_66acc.pt\"))\n",
    "face_model.eval()\n",
    "\n",
    "\n",
    "scripted_module = torch.jit.script(face_model)\n",
    "torchscript_model_optimized = optimize_for_mobile(scripted_module)\n",
    "# Export lite interpreter version model (compatible with lite interpreter)\n",
    "scripted_module._save_for_lite_interpreter(\"Face_CNN.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
